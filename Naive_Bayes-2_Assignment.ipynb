{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996db445",
   "metadata": {},
   "source": [
    "# Naive Bayes-2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99206659",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068875f",
   "metadata": {},
   "source": [
    "This is because we are missing key pieces of information:\n",
    "\n",
    "The overall percentage of smokers in the company (P(Smoker))\n",
    "The probability of someone with health insurance being a smoker (P(Health Insurance | Smoker))\n",
    "Bayes' theorem could be used to calculate the desired probability, but it requires  both these values which we don't have.\n",
    "\n",
    "For instance, even though we know 40% of people with health insurance are smokers (P(Smoker | Health Insurance)), we don't know what percentage of the overall smoker population has health insurance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb868adb",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d0d84",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes classification algorithm, but they differ in how they handle features:\n",
    "\n",
    "Multinomial Naive Bayes: This is the more commonly used variant. It assumes features are discrete and represented by counts. For example, in text classification, features could be word counts in a document. Multinomial Naive Bayes considers the frequency of a feature's occurrence, meaning it cares about how many times a feature appears.\n",
    "\n",
    "Bernoulli Naive Bayes: This variant is suited for features with binary values (0 or 1). It focuses on the presence or absence of a feature, not its frequency. Imagine classifying emails as spam based on keywords. Bernoulli Naive Bayes would just consider if a specific keyword exists in the email (1) or not (0).\n",
    "\n",
    "Here's a table summarizing the key differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de10b3",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21b7fd",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes ignores missing features during training and prediction.\n",
    "This can be a drawback as it discards information.\n",
    "Consider pre-processing data (removing rows/imputation) or using ensemble methods for better handling of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc10d97",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ce6d2",
   "metadata": {},
   "source": [
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification.\n",
    "\n",
    "It doesn't have inherent limitations to binary problems. The algorithm applies Bayes' theorem to calculate probabilities for each class given the features. This approach works for multiple classes as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b067476",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38ccc6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[1;32m----> 5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Load features and target labels\u001b[39;00m\n\u001b[0;32m      8\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m [BernoulliNB(), MultinomialNB(), GaussianNB()]\n\u001b[0;32m     11\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X, y = ...  # Load features and target labels\n",
    "\n",
    "\n",
    "classifiers = [BernoulliNB(), MultinomialNB(), GaussianNB()]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "results = []\n",
    "for classifier in classifiers:\n",
    "    metrics = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "       \n",
    "print(\"Performance Results:\")\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    print(f\"{classifier.__class__.__name__}:\")\n",
    "    print(f\"\\tAccuracy: {results[i]['accuracy']:.4f}\")\n",
    "    print(f\"\\tPrecision: {results[i]['precision']:.4f}\")\n",
    "    print(f\"\\tRecall: {results[i]['recall']:.4f}\")\n",
    "    print(f\"\\tF1 Score: {results[i]['f1']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bc2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
