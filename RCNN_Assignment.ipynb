{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8bb320",
   "metadata": {},
   "source": [
    "# RCNN Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e21373",
   "metadata": {},
   "source": [
    "Q.1.)  What are the objectives using Selective Search in R-CSSP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f0322",
   "metadata": {},
   "source": [
    "Region Proposal Generation (RP): \n",
    "    \n",
    "Pedestrian SVM Model: \n",
    "\n",
    "Clean-up:\n",
    "\n",
    "Implementation of Counting Logic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c52cb",
   "metadata": {},
   "source": [
    "Q.2.) Explain the following phases invlved in R-CSSk Revion p4opoM1 :k W4pinv nt ReMizinw\n",
    "[k P4e t4inet CNN 4chitectu4f\n",
    "uk P4e T4inet SVM ?otelL\n",
    "fk Clen up\n",
    "f. explain entition of ;ountinv ;og"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdc9d4",
   "metadata": {},
   "source": [
    "RP (Region Proposal Generation): Proposes potential regions in an image for pedestrian detection.\n",
    "\n",
    "FE (Feature Extraction): Extracts relevant features from proposed regions using a pre-trained CNN.\n",
    "\n",
    "PDCA (Pedestrian Detection CNN Architecture): Designs and implements a CNN architecture tailored for pedestrian detection.\n",
    "\n",
    "Pedestrian SVM Model: Trains SVM models to classify proposed regions as containing pedestrians or not.\n",
    "\n",
    "Clean-up: Refines detected pedestrian regions to remove redundancy or overlap.\n",
    "\n",
    "Counting Logic Implementation: Implements logic to estimate pedestrian count from detected regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac555118",
   "metadata": {},
   "source": [
    "3.) What are the pssixle pre trained CSSs we can use in Pre trained CSS architectureP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c836d6",
   "metadata": {},
   "source": [
    "VGG (Visual Geometry Group)\n",
    "\n",
    "Simple architecture with deep convolutional layers\n",
    "Strong performance in image classification tasks\n",
    "ResNet (Residual Network)\n",
    "\n",
    "Introduces residual connections to address vanishing gradients\n",
    "Enables training of very deep networks effectively\n",
    "Inception (GoogLeNet)\n",
    "\n",
    "Utilizes parallel convolutional pathways for feature extraction\n",
    "Efficiently captures features at different scales\n",
    "MobileNet\n",
    "\n",
    "Achieves performance improvements through balanced scaling of depth, width, and resolution\n",
    "Provides models suitable for various computational resources\n",
    "DenseNet (Densely Connected Convolutional Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5076a8f",
   "metadata": {},
   "source": [
    "Q.4) How is SVM l implemented in the R-CSS\n",
    "\n",
    "ramewr P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc807f59",
   "metadata": {},
   "source": [
    "Feature Extraction: \n",
    "Training SVM Classifier: \n",
    "Classification of Regions:\n",
    "Integration with Detection Pipeline:  \n",
    "Refinement and Post-processing:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06442a",
   "metadata": {},
   "source": [
    "Q.5) How does non-maximum Suppressin work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a702ed6",
   "metadata": {},
   "source": [
    "Non-maximum suppression (NMS) is a technique used in object detection to eliminate redundant bounding boxes. In R-CSS, after proposing regions and extracting features, NMS is applied by selecting the region with the highest confidence score among overlapping regions. This helps to remove duplicate detections and refine the final output, ensuring that only the most confident and non-overlapping pedestrian regions are retained for further processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf9260",
   "metadata": {},
   "source": [
    "Q.6) How Fast R-CSS is better than R-CNN ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7f787",
   "metadata": {},
   "source": [
    "Fast R-CSS is preferred for applications where speed is critical and a slight reduction in accuracy is acceptable, while R-CSSP is suitable for tasks where higher accuracy is paramount, even at the expense of some speed. The choice between the two depends on the specific requirements and constraints of the application.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec98b62",
   "metadata": {},
   "source": [
    "Q.7) Using mathematical intuitin, explain ROI polling in Fast R-CSS ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee27efe",
   "metadata": {},
   "source": [
    "R \n",
    "I\n",
    "  pooling in Fast R-CSS ensures that features extracted from proposed regions are consistently represented in fixed-size feature maps, enabling effective and efficient processing for pedestrian detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6de95",
   "metadata": {},
   "source": [
    "Q.8) Explain the follwing prcesses\n",
    "\n",
    "1. ROI P4ojection\n",
    "2. ROI poolinw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a990c",
   "metadata": {},
   "source": [
    " ROI projection and ROI pooling are essential processes in object detection frameworks like Fast R-CNN and Faster R-CNN. They enable the alignment, localization, and feature extraction of proposed regions within the context of convolutional neural networks, leading to accurate and efficient object detection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affc863",
   "metadata": {},
   "source": [
    "Q.9) In cmparisn with R-CSS, wh did the xject classicler activatin unctin change in Fast R-CSSP ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b48fe",
   "metadata": {},
   "source": [
    " Fast R-CSSP can achieve faster inference times while still maintaining adequate performance for pedestrian detection tasks. However, the specific choice of activation function may vary depending on the exact requirements and constraints of the application.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9599da",
   "metadata": {},
   "source": [
    "10.)  What major changes in Faster R-CSS cmpared t Fast R-CSSP ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b31321",
   "metadata": {},
   "source": [
    "Region Proposal Network (RPN):\n",
    "End-to-End Training:\n",
    "Feature Sharing:\n",
    "Anchor Boxes:\n",
    "Training Strategies:\n",
    "Faster R-CSS employs advanced training strategies, such as hard negative mining and online hard example mining, to improve the training process and address class imbalance issues.\n",
    "Fast R-CSSP may use simpler training strategies or may not fully optimize the training process for efficiency and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfd0dc",
   "metadata": {},
   "source": [
    "Q.11) Explain the concept Anchor box ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfcb81",
   "metadata": {},
   "source": [
    "\n",
    "The concept of anchor boxes, also known as anchor boxes or default boxes, is fundamental in object detection tasks, particularly in frameworks like Faster R-CNN and its variants. Anchor boxes are predefined bounding boxes of various sizes and aspect ratios that are placed at different locations across an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd438f",
   "metadata": {},
   "source": [
    "12.)  Implement Faster R-CSS using 20\\Q C^C^ dataset  i.e. Train dataset, Val dataset and Test dataset. Yu can use a pre-trained xac xne netwr li e ResSet r VGG\n",
    "eature extractin. Fr re\n",
    "erence implement the\n",
    "follwing steps :\n",
    "a. Dataset Preparatin\n",
    "i. Download and preprcess the concern dataset, including the anntatins and images.\n",
    "ii. Split the dataset int training and validatin sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc59c830",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/path/to/preprocessed_dataset\\\\images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Load images from images_path\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Split dataset into images and annotations\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(images_path))\n\u001b[0;32m     16\u001b[0m annotation_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(annotations_path))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/path/to/preprocessed_dataset\\\\images'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = \"/path/to/preprocessed_dataset\"\n",
    "annotations_path = os.path.join(dataset_path, \"annotations\")\n",
    "images_path = os.path.join(dataset_path, \"images\")\n",
    "\n",
    "annotations = ...  \n",
    "images = ...  \n",
    "\n",
    "image_files = sorted(os.listdir(images_path))\n",
    "annotation_files = sorted(os.listdir(annotations_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f93e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
