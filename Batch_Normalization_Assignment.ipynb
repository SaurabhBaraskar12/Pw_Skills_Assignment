{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecaed2e1",
   "metadata": {},
   "source": [
    "# Batch Normalization Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fd43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80f7e9",
   "metadata": {},
   "source": [
    "Q.1) Explain the concept of batch normalization in the context of Artificial Neural Networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116842",
   "metadata": {},
   "source": [
    "\n",
    "Batch normalization is a technique used in neural networks to stabilize and accelerate training by normalizing the activations of each layer. It reduces internal covariate shift by normalizing the inputs and scaling them using learnable parameters. This leads to faster convergence, improved stability, and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d289fe3",
   "metadata": {},
   "source": [
    "Q.2) Describe the benefits of using batch normalization during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c6130",
   "metadata": {},
   "source": [
    "\n",
    "Batch normalization during training provides faster convergence, stabilizes training by reducing sensitivity to weight initialization and hyperparameters, acts as a form of regularization to prevent overfitting, ensures consistent gradient flow, and reduces dependency on weight initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fb130",
   "metadata": {},
   "source": [
    "Q.3) Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
    "parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480f054",
   "metadata": {},
   "source": [
    "batch normalization helps stabilize training, accelerate convergence, and improve the generalization performance of neural networks by reducing internal covariate shift and allowing the network to learn the optimal scaling and shifting of activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da41d8",
   "metadata": {},
   "source": [
    "# implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c2fd9",
   "metadata": {},
   "source": [
    "Q.1) Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess itr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68072eb5",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e601ac",
   "metadata": {},
   "source": [
    "Q.2) Implementation a simple feedforward neural network using any deep learning framework/library (e.g.,\n",
    "Tensorlow, xyTorch)r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1f90d",
   "metadata": {},
   "source": [
    "We define a sequential model using tf.keras.models.Sequential, which allows us to stack layers sequentially.\n",
    "The model consists of:\n",
    "Flatten layer to flatten the input images from 28x28 to a 1D array.\n",
    "Dense layer with 128 units and ReLU activation.\n",
    "Dropout layer with a dropout rate of 0.2 to prevent overfitting.\n",
    "Dense output layer with 10 units for the 10 classes in the MNIST dataset and softmax activation.\n",
    "We compile the model using the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric.\n",
    "Finally, we print a summary of the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9c1bb",
   "metadata": {},
   "source": [
    "Q.3) Train the neural network on the chosen dataset without using batch normalizationr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56cad73",
   "metadata": {},
   "source": [
    "We load the MNIST dataset and normalize the pixel values to the range [0, 1].\n",
    "We define the same feedforward neural network architecture as before, without batch normalization layers.\n",
    "The model is compiled with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric.\n",
    "We train the model on the training data for 5 epochs, using the validation data for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910cdc4",
   "metadata": {},
   "source": [
    "Q.4)Implement batch normalization layers in the neural network and train the model againr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56798e2",
   "metadata": {},
   "source": [
    "We define the same feedforward neural network architecture as before, but we add a batch normalization layer after the first dense layer.\n",
    "The batch normalization layer normalizes the activations of the previous layer.\n",
    "The model is compiled and trained using the same settings as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d60986",
   "metadata": {},
   "source": [
    "Q.5) Compare the training and validation performance (e.g., accuracy, loss) between the models with and\n",
    "without batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501cc9d",
   "metadata": {},
   "source": [
    "We train two models: one without batch normalization (model_without_bn) and one with batch normalization (model_with_bn).\n",
    "We compare the training and validation performance (accuracy and loss) between the two models by printing their history dictionaries (history_without_bn and history_with_bn)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c89b4",
   "metadata": {},
   "source": [
    "Q.6) Discuss the impact of batch normalization on the training process and the performance of the neural\n",
    "network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f1423",
   "metadata": {},
   "source": [
    "batch normalization is a powerful technique for training deep neural networks more efficiently and effectively. It addresses several challenges associated with training deep networks and contributes to improved performance and stability across a wide range of architectures and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810f710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
